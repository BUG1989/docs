## RKLLM Introduction

RKLLM helps users quickly deploy LLM models onto Rockchip chips. Currently supported chips are: RK3588/RK3576/RK3562 series chips.

The overall framework of RKLLM is as follows:

![rkllm_1.webp](/img/general-tutorial/rknn/rkllm_1.webp)

#### Currently Supported Models

- [LLAMA models](https://huggingface.co/meta-llama)
- [TinyLLAMA models](https://huggingface.co/TinyLlama)
- [Qwen models](https://huggingface.co/models?search=Qwen/Qwen)
- [Phi models](https://huggingface.co/models?search=microsoft/phi)
- [ChatGLM3-6B](https://huggingface.co/THUDM/chatglm3-6b/tree/103caa40027ebfd8450289ca2f278eac4ff26405)
- [Gemma2](https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315)
- [Gemma3](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d)
- [InternLM2 models](https://huggingface.co/collections/internlm/internlm2-65b0ce04970888799707893c)
- [MiniCPM models](https://huggingface.co/collections/openbmb/minicpm-65d48bf958302b9fd25b698f)
- [TeleChat models](https://huggingface.co/Tele-AI)
- [Qwen2-VL-2B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)
- [MiniCPM-V-2_6](https://huggingface.co/openbmb/MiniCPM-V-2_6)
- [DeepSeek-R1-Distill](https://huggingface.co/collections/deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d)
- [Janus-Pro-1B](https://huggingface.co/deepseek-ai/Janus-Pro-1B)
- [InternVL2-1B](https://huggingface.co/OpenGVLab/InternVL2-1B)
- [Qwen2.5-VL-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)
- [Qwen3](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)

## RKLLM Installation

To use RKNPU, users need to first run the RKLLM-Toolkit on an x86 workstation to convert trained models into RKLLM format, and then perform inference on the development board using the RKLLM C API.

### x86 PC Workstation

- (Optional) Install [Anaconda](https://www.anaconda.com/)

  If Python 3.11 (required version) is not installed in your system or you have multiple Python environments, it is recommended to use [Anaconda](https://www.anaconda.com/) to create a new Python 3.11 environment.

  - Install Anaconda

    Execute the following command in the terminal window of your computer to check whether Anaconda is already installed. If yes, skip this section.

    <NewCodeBlock tip="X86 Linux PC" type="PC">

    ```bash
    $ conda --version
    conda 24.9.2
    ```

    </NewCodeBlock>

    If you see "conda: command not found", it means Anaconda is not installed. Please refer to the [Anaconda](https://www.anaconda.com/) official website for installation instructions.

  - Create a conda environment

    <NewCodeBlock tip="X86 Linux PC" type="PC">

    ```bash
    conda create -n rkllm python=3.11.11
    ```

    </NewCodeBlock>

  - Enter the rkllm conda environment

    <NewCodeBlock tip="X86 Linux PC" type="PC">

    ```bash
    conda activate rkllm
    ```

    </NewCodeBlock>

  - _To exit the environment_

    <NewCodeBlock tip="X86 Linux PC" type="PC">

    ```bash
    conda deactivate
    ```

    </NewCodeBlock>

- Clone the RKLLM repository

  <NewCodeBlock tip="X86 Linux PC" type="PC">

  ```bash
  git clone -b release-v1.2.1b1 https://github.com/airockchip/rknn-llm.git   && cd rknn-llm
  ```

  </NewCodeBlock>

- Install RKLLM-Toolkit

  RKLLM-Toolkit is a software development kit that allows users to quantize and convert Huggingface-format LLM models on X86 PCs.

  <NewCodeBlock tip="X86 Linux PC" type="PC">

  ```bash
  pip3 install ./rkllm-toolkit/rkllm_toolkit-1.2.1b1-cp311-cp311-linux_x86_64.whl
  ```

  </NewCodeBlock>

  If no errors occur when executing the following commands, the installation was successful.

  <NewCodeBlock tip="X86 Linux PC" type="PC">

  ```bash
  $python3
  >>>from rkllm.api import RKLLM
  ```

  </NewCodeBlock>

### Development Board

- Check whether the RKNPU driver version is at least 0.9.8. If it is lower than this version, download and flash the latest radxa 6.1 firmware.
  :::tip
  The default RKNPU driver version in the radxa 6.1 firmware is 0.9.6. Please update to version 0.9.8 via: `sudo rsetup -> System -> System Update`.
  After the update, be sure to execute **`sudo apt autopurge`** and then reboot.
  :::

  <NewCodeBlock tip="Radxa OS" type="device">

  ```bash
  $ sudo cat /sys/kernel/debug/rknpu/version
  RKNPU driver: v0.9.8
  ```

  </NewCodeBlock>

- (Optional) Manually compile the NPU kernel

  If you are using a non-official firmware, you may need to update the kernel. The RKNPU driver supports two main kernel versions: [kernel-5.10](https://github.com/radxa/kernel/tree/stable-5.10-rock5) and [kernel-6.1](https://github.com/radxa/kernel/tree/linux-6.1-stan-rkr1). You can confirm the specific version number in the Makefile at the root directory of the kernel. The specific steps to update the kernel are as follows:

  1. Download the archive file [rknpu_driver_0.9.8_20241009.tar.bz2](https://github.com/airockchip/rknn-llm/tree/release-v1.2.1b1/rknpu-driver).

  2. Extract the archive and replace the rknpu driver code in the current kernel source directory with it.

  3. Recompile the kernel.

  4. Flash the newly compiled kernel to the device.

- RKLLM Runtime provides C/C++ programming interfaces for the Rockchip NPU platform, helping users deploy RKLLM models and accelerate the implementation of LLM applications. Clone the RKLLM repository on the device side.

  <NewCodeBlock tip="Radxa OS" type="device">

  ```bash
  git clone -b release-v1.2.1b1 https://github.com/airockchip/rknn-llm.git
  ```

  </NewCodeBlock>
