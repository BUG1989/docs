## Introduction to RKLLM

RKLLM helps users deploy LLM models to Rockchip chips quickly. Currently, it supports the rk3588/rk3576 chips. The overall framework is shown below:

![rkllm_1.webp](/img/general-tutorial/rknn/rkllm_1.webp)

### Currently Supported Models

- [LLAMA models](https://huggingface.co/meta-llama)
- [TinyLLAMA models](https://huggingface.co/TinyLlama)
- [Qwen models](https://huggingface.co/models?search=Qwen/Qwen)
- [Phi models](https://huggingface.co/models?search=microsoft/phi)
- [ChatGLM3-6B](https://huggingface.co/THUDM/chatglm3-6b/tree/103caa40027ebfd8450289ca2f278eac4ff26405)
- [Gemma models](https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315)
- [InternLM2 models](https://huggingface.co/collections/internlm/internlm2-65b0ce04970888799707893c)
- [MiniCPM models](https://huggingface.co/collections/openbmb/minicpm-65d48bf958302b9fd25b698f)
- [TeleChat models](https://huggingface.co/Tele-AI)
- [Qwen2-VL](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)
- [MiniCPM-V](https://huggingface.co/openbmb/MiniCPM-V-2_6)

## Install RKLLM

To use RKNPU, users first need to run the RKLLM-Toolkit tool on an x86 workstation to convert the trained model to the RKLLM format, then use the RKLLM C API on the development board for inference.

### x86 PC Workstation

- (Optional) Install [Anaconda](https://www.anaconda.com/)

  If Python 3.8 (required version) is not installed on your system or if you have multiple Python versions, it is recommended to use [Anaconda](https://www.anaconda.com/) to create a new Python 3.8 environment.

  - Install Anaconda

    To check if Anaconda is installed, run the following command in the terminal. If it is installed, you can skip this step.

    ```bash
    $ conda --version
    conda 23.10.0
    ```

    If you see `conda: command not found`, Anaconda is not installed. Please refer to the [Anaconda](https://www.anaconda.com/) website for installation instructions.

  - Create a conda environment

    ```bash
    conda create -n rkllm python=3.8.2
    ```

  - Activate the rkllm conda environment

    ```bash
    conda activate rkllm
    ```

  - Deactivate the environment

    ```bash
    conda deactivate
    ```

- RKLLM-Toolkit is a software development package for converting and quantizing Huggingface format LLM models on a PC.

  ```bash
  git clone -b release-v1.1.4 https://github.com/airockchip/rknn-llm.git
  pip3 install ./rknn-llm/rkllm-toolkit/packages/rkllm_toolkit-1.1.4-cp38-cp38-linux_x86_64.whl
  ```

  If the following command runs without errors, the installation is successful:

  ```bash
  python3
  from rkllm.api import RKLLM
  ```

### Development Board

- Check if the NPU driver version is 0.9.6 or higher. If it is lower, download and flash the latest Radxa 6.1 firmware.
  :::tip
  The default NPU driver version of radxa 6.1 firmware is 0.9.6, please update the system to 0.9.8 driver by: `sudo rsetup -> System -> System Update`.
  :::

  ```bash
  $ sudo cat /sys/kernel/debug/rknpu/version
  RKNPU driver: v0.9.8
  ```

  (Optional) Manually compile the NPU kernel

  If you are using a non-official firmware, you need to update the kernel. The RKNPU driver package supports two major kernel versions: [kernel-5.10](https://github.com/radxa/kernel/tree/stable-5.10-rock5) and [kernel-6.1](https://github.com/radxa/kernel/tree/linux-6.1-stan-rkr1). You can confirm the exact version number in the Makefile in the root directory of the kernel. Follow these steps to update the kernel:

  1. Download the [rknpu_driver_0.9.6_20240322.tar.bz2](https://github.com/airockchip/rknn-llm/tree/main/rknpu-driver) archive.

  2. Extract the archive and overwrite the current kernel code directory with the rknpu driver code.

  3. Recompile the kernel.

  4. Flash the newly compiled kernel to the device.

- RKLLM Runtime provides C/C++ programming interfaces for the Rockchip NPU platform to help users deploy RKLLM models and accelerate LLM applications.

  ```bash
  git clone -b release-v1.1.4 https://github.com/airockchip/rknn-llm.git
  ```
