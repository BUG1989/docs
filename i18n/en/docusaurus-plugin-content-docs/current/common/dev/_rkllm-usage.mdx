This document explains how to deploy large language models in Huggingface format to the RK3588 with NPU for hardware-accelerated inference using RKLLM.

#### Currently Supported Models

- [LLAMA models](https://huggingface.co/meta-llama)
- [TinyLLAMA models](https://huggingface.co/TinyLlama)
- [Qwen models](https://huggingface.co/models?search=Qwen/Qwen)
- [Phi models](https://huggingface.co/models?search=microsoft/phi)
- [ChatGLM3-6B](https://huggingface.co/THUDM/chatglm3-6b/tree/103caa40027ebfd8450289ca2f278eac4ff26405)
- [Gemma models](https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315)
- [InternLM2 models](https://huggingface.co/collections/internlm/internlm2-65b0ce04970888799707893c)
- [MiniCPM models](https://huggingface.co/collections/openbmb/minicpm-65d48bf958302b9fd25b698f)
- [TeleChat models](https://huggingface.co/Tele-AI)
- [Qwen2-VL](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)
- [MiniCPM-V](https://huggingface.co/openbmb/MiniCPM-V-2_6)

This guide uses [Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct) as an example to show how to deploy a large language model from scratch on a development board equipped with the RK3588 chip and use the NPU for hardware-accelerated inference.

:::tip
If the RKLLM environment is not installed and configured, please refer to [RKLLM Installation](rkllm_install).
:::

### Model Conversion

Using [Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct) as an example, users can also choose any of the links in the [currently supported models](#currently-supported-models) list.

- Download all files of [Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct) on an x86 PC workstation. If [git-lfs](https://git-lfs.com/) is not installed, please install it.
  ```bash
  git clone https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct
  ```
- Activate the rkllm conda environment. Refer to [RKLLM conda Installation](rkllm_install#x86-pc-workstation) if needed.
  ```bash
  conda activate rkllm
  ```
- Change modelpath model path, dataset path, rkllm export path in `rknn-llm/rkllm-toolkit/examples/test.py`.
  ```python
  15 modelpath = 'Your Huggingface LLM model'
  29 datasert = None # 默认是 "./data_quant.json"， 如无可以填写 None
  83 ret = llm.export_rkllm("./Your_Huggingface_LLM_model.rkllm")
  ```
- Run the model conversion script.
  ```bash
  cd rknn-llm/rkllm-toolkit/examples/
  python3 test.py
  ```
  After successful conversion, you will get an rkllm model.

### Compile Executable File

- Download the cross-compilation toolchain [gcc-arm-10.2-2020.11-x86_64-aarch64-none-linux-gnu](https://developer.arm.com/downloads/-/gnu-a/10-2-2020-11).
- Modify the main program `rknn-llm/examples/rkllm_api_demo/src/llm_demo.cpp` code, change two places here.
  ```vim
  184 text = PROMPT_TEXT_PREFIX + input_str + PROMPT_TEXT_POSTFIX;
  185 // text = input_str;
  ```
- Modify the GCC_COMPILER_PATH in the `rknn-llm/examples/rkllm_api_demo/build-linux.sh` compilation script.
  ```bash
  GCC_COMPILER_PATH=gcc-arm-10.2-2020.11-x86_64-aarch64-none-linux-gnu/bin/aarch64-none-linux-gnu
  ```
- Run the model conversion script.
  ```bash
  cd rknn-llm/examples/rkllm_api_demo/
  bash build-linux.sh
  ```
  The generated executable file is located in `build/build_linux_aarch64_Release/llm_demo`.

### Board Deployment

#### Terminal Mode

- Copy the converted rkllm model and the compiled binary file llm_demo to the board.
- Import environment variables.
  ```bash
  export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:rknn-llm/rkllm-runtime/Linux/librkllm_api/aarch64
  ```
- Run llm_demo and enter `exit` to quit.
  ```bash
  export RKLLM_LOG_LEVEL=1
  ./llm_demo your_rkllm_path 10000 10000
  ```
  ![rkllm_2.webp](/img/general-tutorial/rknn/rkllm_2.webp)

{/* #### Gradio Mode */}

{/* ##### Server Side */}

{/* - Install gradio. */}
{/* ```bash */}
{/* pip3 install gradio */}
{/* ``` */}
{/* - Copy `librkllmrt.so` to `rkllm_server/lib`. */}
{/* ```bash */}
{/* cd rknn-llm/rkllm-runtime */}
{/* cp ./runtime//Linux/librkllm_api/aarch64/librkllmrt.so  ./examples/rkllm_server_demo/rkllm_server/lib */}
{/* ``` */}
{/* - Modify gradio_server.py to disable GPU for prefill acceleration. */}
{/* ```python */}
{/* rknnllm_param.use_gpu = False */}
{/* ``` */}
{/* - Start the gradio server. */}
{/* ```bash */}
{/* cd examples/rkllm_server_demo/rkllm_server */}
{/* python3 gradio_server.py --target_platform rk3588 --rkllm_model_path your_model_path */}
{/* ``` */}
{/* - Access the development board's IP port 8080 in your browser. */}
{/* ![rkllm_3.webp](/img/general-tutorial/rknn/rkllm_3.webp) */}

{/* ##### Client Side */}

{/* After starting the gradio server on the development board, users can call the LLM gradio server through the Gradio API on other devices in the same network environment. */}

{/* - Install gradio_client. */}
{/* ```bash */}
{/* pip3 install gradio_client */}
{/* ``` */}
{/* - Modify the IP address in chat_api_gradio.py. Users need to adjust this according to their deployment's specific address. */}
{/* ```python */}
{/* # Users need to modify according to their deployment's specific IP */}
{/* client = Client("http://192.168.2.209:8080/") */}
{/* ``` */}
{/* - Run chat_api_gradio.py. */}
{/* ```bash */}
{/* cd rknn-llm/rkllm-runtime/examples/rkllm_server_demo */}
{/* python3 chat_api_gradio.py */}
{/* ``` */}
{/* ![rkllm_4.webp](/img/general-tutorial/rknn/rkllm_4.webp) */}

{/* #### Flask Mode */}

{/* ##### Server Side */}

{/* - Install flask. */}
{/* ```bash */}
{/* pip3 install flask==2.2.2 Werkzeug==2.2.2 */}
{/* ``` */}
{/* - Copy `librkllmrt.so` to `rkllm_server/lib`. */}
{/* ```bash */}
{/* cd rknn-llm/rkllm-runtime */}
{/* cp ./runtime//Linux/librkllm_api/aarch64/librkllmrt.so  ./examples/rkllm_server_demo/rkllm_server/lib */}
{/* ``` */}
{/* - Modify flask_server.py to disable GPU for prefill acceleration. */}
{/* ```python */}
{/* rknnllm_param.use_gpu = False */}
{/* ``` */}
{/* - Start the flask server on port 8080. */}
{/* ```bash */}
{/* cd examples/rkllm_server_demo/rkllm_server */}
{/* python3 flask_server.py --target_platform rk3588 --rkllm_model_path your_model_path */}
{/* ``` */}
{/* ![rkllm_5.webp](/img/general-tutorial/rknn/rkllm_5.webp) */}

{/* ##### Client Side */}

{/* After starting the flask server on the development board, users can call the flask server through the flask API on other devices in the same network environment. Users can refer to this API access example to develop custom functions, using the corresponding send/receive structures for data packaging and parsing. */}

{/* - Modify the IP address in chat_api_flask.py. Users need to adjust this according to their deployment's specific address. */}
{/* ```python */}
{/* # Users need to modify according to their deployment's specific IP */}
{/* server_url = 'http://192.168.2.209:8080/rkllm_chat' */}
{/* ``` */}
{/* - Run chat_api_flask.py. */}
{/* ```bash */}
{/* cd rknn-llm/rkllm-runtime/examples/rkllm_server_demo */}
{/* python3 chat_api_flask.py */}
{/* ``` */}
{/* ![rkllm_6.webp](/img/general-tutorial/rknn/rkllm_6.webp) */}

### Performance Comparison of Models

| Model     | Parameter Size | Chip   | Chip Count | Inference Speed |
| --------- | -------------- | ------ | ---------- | --------------- |
| TinyLlama | 1.1B           | RK3588 | 1          | 15.03 token/s   |
| Qwen      | 1.8B           | RK3588 | 1          | 14.18 token/s   |
| Phi3      | 3.8B           | RK3588 | 1          | 6.46 token/s    |
| ChatGLM3  | 6B             | RK3588 | 1          | 3.67 token/s    |
